"""
ComfyUI Whisperæ¨¡å‹åŠ è½½èŠ‚ç‚¹
å•ç‹¬ç®¡ç†Whisperæ¨¡å‹çš„åŠ è½½å’Œç¼“å­˜
"""

import os
import sys
import logging
from typing import Dict, Any, Tuple, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ä»¥æ”¯æŒå¯¼å…¥
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# å¯¼å…¥æœåŠ¡
try:
    from ..services.whisper_service import WhisperService
except ImportError:
    try:
        from services.whisper_service import WhisperService
    except ImportError:
        from whisper_service import WhisperService


class WhisperModelNode:
    """Whisperæ¨¡å‹åŠ è½½èŠ‚ç‚¹"""
    
    # å…¨å±€æ¨¡å‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
    _model_cache = {}
    
    def __init__(self):
        pass
        
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        return {
            "required": {
                "model_size": ([
                    "tiny", "base", "small", "medium", 
                    "large", "large-v2", "large-v3"
                ], {
                    "default": "large-v3",
                    "tooltip": "Whisperæ¨¡å‹å¤§å°ï¼Œè¶Šå¤§è¶Šå‡†ç¡®ä½†éœ€è¦æ›´å¤šå†…å­˜"
                }),
                "device": (["cuda", "cpu"], {
                    "default": "cuda",
                    "tooltip": "è®¡ç®—è®¾å¤‡ï¼ŒGPUåŠ é€Ÿéœ€è¦CUDAæ”¯æŒ"
                }),
                "compute_type": ([
                    "float16", "float32", "int8", "int16"
                ], {
                    "default": "float16",
                    "tooltip": "è®¡ç®—ç²¾åº¦ï¼Œfloat16èŠ‚çœå†…å­˜ï¼Œint8æ›´å¿«ä½†ç²¾åº¦ç¨ä½"
                })
            },
            "optional": {
                "force_reload": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹ï¼Œå³ä½¿å·²ç¼“å­˜"
                })
            }
        }
    
    RETURN_TYPES = ("WHISPER_MODEL", "STRING")
    RETURN_NAMES = ("whisper_model", "model_info")
    FUNCTION = "load_model"
    CATEGORY = "Audio/Whisper"
    
    def load_model(self, model_size: str, device: str, compute_type: str, 
                   force_reload: bool = False) -> Tuple[WhisperService, str]:
        """
        åŠ è½½Whisperæ¨¡å‹
        
        Args:
            model_size: æ¨¡å‹å¤§å°
            device: è®¡ç®—è®¾å¤‡
            compute_type: è®¡ç®—ç±»å‹
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
            
        Returns:
            (WhisperServiceå®ä¾‹, æ¨¡å‹ä¿¡æ¯å­—ç¬¦ä¸²)
        """
        try:
            # åˆ›å»ºæ¨¡å‹æ ‡è¯†ç¬¦
            model_key = f"{model_size}_{device}_{compute_type}"
            
            # æ£€æŸ¥ç¼“å­˜
            if not force_reload and model_key in self._model_cache:
                cached_service = self._model_cache[model_key]
                model_info = f"âœ… ä½¿ç”¨ç¼“å­˜æ¨¡å‹: {model_size} ({device}, {compute_type})"
                return cached_service, model_info
            
            # åˆ›å»ºæ–°çš„WhisperServiceå®ä¾‹
            whisper_service = WhisperService()
            
            # é¢„åŠ è½½æ¨¡å‹ä»¥éªŒè¯å¯ç”¨æ€§
            model_info_lines = [
                f"ğŸ”„ åŠ è½½Whisperæ¨¡å‹...",
                f"æ¨¡å‹å¤§å°: {model_size}",
                f"è®¡ç®—è®¾å¤‡: {device}",
                f"è®¡ç®—ç±»å‹: {compute_type}"
            ]
            
            # å°è¯•åŠ è½½æ¨¡å‹
            try:
                # é€šè¿‡è°ƒç”¨å†…éƒ¨æ–¹æ³•é¢„åŠ è½½æ¨¡å‹
                model = whisper_service._load_model(model_size, device, compute_type)
                
                # è·å–æ¨¡å‹ä¿¡æ¯
                model_info_lines.extend([
                    f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ",
                    f"æ¨¡å‹ç±»å‹: {type(model).__name__}",
                    f"ç¼“å­˜é”®: {model_key}"
                ])
                
                # ç¼“å­˜æ¨¡å‹æœåŠ¡
                self._model_cache[model_key] = whisper_service
                
                model_info = "\n".join(model_info_lines)
                return whisper_service, model_info
                
            except Exception as e:
                error_info = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                model_info_lines.append(error_info)
                model_info = "\n".join(model_info_lines)
                
                # è¿”å›Noneå’Œé”™è¯¯ä¿¡æ¯
                return None, model_info
                
        except Exception as e:
            error_msg = f"âŒ èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯: {str(e)}"
            return None, error_msg
    
    @classmethod
    def clear_cache(cls):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„æ¨¡å‹"""
        for service in cls._model_cache.values():
            if hasattr(service, 'clear_model_cache'):
                service.clear_model_cache()
        cls._model_cache.clear()
    
    @classmethod
    def get_cache_info(cls) -> str:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        if not cls._model_cache:
            return "ğŸ“­ æ²¡æœ‰ç¼“å­˜çš„æ¨¡å‹"
        
        info_lines = ["ğŸ“¦ ç¼“å­˜çš„æ¨¡å‹:"]
        for key in cls._model_cache.keys():
            info_lines.append(f"  - {key}")
        
        return "\n".join(info_lines)


class WhisperTranscribeNode:
    """WhisperéŸ³é¢‘è½¬å½•èŠ‚ç‚¹"""
    
    def __init__(self):
        pass
        
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        return {
            "required": {
                "whisper_model": ("WHISPER_MODEL", {
                    "tooltip": "ä»Whisperæ¨¡å‹åŠ è½½èŠ‚ç‚¹è·å–çš„æ¨¡å‹"
                }),
                "audio_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "éŸ³é¢‘æ–‡ä»¶è·¯å¾„"
                })
            },
            "optional": {
                "language": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "æŒ‡å®šè¯­è¨€ä»£ç (å¦‚:zh,en)ï¼Œç•™ç©ºè‡ªåŠ¨æ£€æµ‹"
                }),
                "beam_size": ("INT", {
                    "default": 5,
                    "min": 1,
                    "max": 10,
                    "step": 1,
                    "tooltip": "æŸæœç´¢å¤§å°ï¼Œè¶Šå¤§è¶Šå‡†ç¡®ä½†è¶Šæ…¢"
                })
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "FLOAT", "STRING")
    RETURN_NAMES = ("transcription", "language", "confidence", "segments_info")
    FUNCTION = "transcribe_audio"
    CATEGORY = "Audio/Whisper"
    
    def transcribe_audio(self, whisper_model: WhisperService, audio_path: str,
                        language: str = "", beam_size: int = 5) -> Tuple[str, str, float, str]:
        """
        ä½¿ç”¨Whisperæ¨¡å‹è½¬å½•éŸ³é¢‘
        
        Args:
            whisper_model: Whisperæ¨¡å‹æœåŠ¡å®ä¾‹
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: æŒ‡å®šè¯­è¨€ï¼ˆå¯é€‰ï¼‰
            beam_size: æŸæœç´¢å¤§å°
            
        Returns:
            (è½¬å½•æ–‡æœ¬, æ£€æµ‹è¯­è¨€, ç½®ä¿¡åº¦, æ®µè½ä¿¡æ¯)
        """
        try:
            # éªŒè¯æ¨¡å‹
            if whisper_model is None:
                error_msg = "âŒ Whisperæ¨¡å‹æœªåŠ è½½æˆ–åŠ è½½å¤±è´¥"
                return "", "", 0.0, error_msg
            
            # éªŒè¯éŸ³é¢‘æ–‡ä»¶
            if not os.path.exists(audio_path):
                error_msg = f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}"
                return "", "", 0.0, error_msg
            
            # æ‰§è¡Œè½¬å½• - ç›´æ¥è°ƒç”¨å†…éƒ¨æ–¹æ³•ï¼Œå› ä¸ºæ¨¡å‹å·²ç»åŠ è½½
            if hasattr(whisper_model, '_model') and whisper_model._model is not None:
                # ç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œè½¬å½•
                try:
                    segments, info = whisper_model._model.transcribe(audio_path, beam_size=beam_size)
                    
                    # æ”¶é›†æ‰€æœ‰æ–‡æ¡ˆ
                    transcript_lines = []
                    full_text = ""
                    
                    for segment in segments:
                        timestamp_line = f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}"
                        transcript_lines.append(timestamp_line)
                        full_text += segment.text + " "
                    
                    result = {
                        'language': info.language,
                        'language_probability': info.language_probability,
                        'segments': transcript_lines,
                        'full_text': full_text.strip()
                    }
                    
                except Exception as e:
                    error_msg = f"âŒ æ¨¡å‹è½¬å½•å¤±è´¥: {str(e)}"
                    return "", "", 0.0, error_msg
            else:
                error_msg = "âŒ æ¨¡å‹æœªæ­£ç¡®åŠ è½½"
                return "", "", 0.0, error_msg
            
            if result is None:
                error_msg = "âŒ éŸ³é¢‘è½¬å½•å¤±è´¥"
                return "", "", 0.0, error_msg
            
            # æå–ç»“æœ
            full_text = result.get('full_text', '')
            language_code = result.get('language', 'unknown')
            confidence = result.get('language_probability', 0.0)
            segments = result.get('segments', [])
            
            # ç”Ÿæˆæ®µè½ä¿¡æ¯
            segments_info_lines = [
                f"ğŸ¯ è½¬å½•å®Œæˆ",
                f"æ£€æµ‹è¯­è¨€: {whisper_model.get_language_name(language_code)} ({language_code})",
                f"ç½®ä¿¡åº¦: {confidence:.2f}",
                f"æ®µè½æ•°: {len(segments)}",
                f"æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦"
            ]
            
            if segments:
                segments_info_lines.append("ğŸ“ å‰3ä¸ªæ®µè½:")
                for i, segment in enumerate(segments[:3], 1):
                    segments_info_lines.append(f"  {i}. {segment}")
                
                if len(segments) > 3:
                    segments_info_lines.append(f"  ... è¿˜æœ‰ {len(segments) - 3} ä¸ªæ®µè½")
            
            segments_info = "\n".join(segments_info_lines)
            
            return full_text, language_code, confidence, segments_info
            
        except Exception as e:
            error_msg = f"âŒ è½¬å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            return "", "", 0.0, error_msg


class WhisperCacheManagerNode:
    """Whisperç¼“å­˜ç®¡ç†èŠ‚ç‚¹"""
    
    def __init__(self):
        pass
        
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        return {
            "required": {
                "action": (["get_info", "clear_cache"], {
                    "default": "get_info",
                    "tooltip": "é€‰æ‹©æ“ä½œï¼šè·å–ç¼“å­˜ä¿¡æ¯æˆ–æ¸…é™¤ç¼“å­˜"
                })
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("cache_info",)
    FUNCTION = "manage_cache"
    CATEGORY = "Audio/Whisper"
    
    def manage_cache(self, action: str) -> Tuple[str]:
        """
        ç®¡ç†Whisperæ¨¡å‹ç¼“å­˜
        
        Args:
            action: æ“ä½œç±»å‹
            
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—ç¬¦ä¸²
        """
        try:
            if action == "get_info":
                cache_info = WhisperModelNode.get_cache_info()
                return (cache_info,)
            elif action == "clear_cache":
                WhisperModelNode.clear_cache()
                return ("ğŸ—‘ï¸ æ‰€æœ‰Whisperæ¨¡å‹ç¼“å­˜å·²æ¸…é™¤",)
            else:
                return (f"âŒ æœªçŸ¥æ“ä½œ: {action}",)
                
        except Exception as e:
            error_msg = f"âŒ ç¼“å­˜ç®¡ç†é”™è¯¯: {str(e)}"
            return (error_msg,)


# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "WhisperModelNode": WhisperModelNode,
    "WhisperTranscribeNode": WhisperTranscribeNode,
    "WhisperCacheManagerNode": WhisperCacheManagerNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "WhisperModelNode": "ğŸ¤– Whisper Model Loader",
    "WhisperTranscribeNode": "ğŸ™ï¸ Whisper Transcribe",
    "WhisperCacheManagerNode": "ğŸ—‚ï¸ Whisper Cache Manager"
}

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•WhisperèŠ‚ç‚¹...")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½èŠ‚ç‚¹
    model_node = WhisperModelNode()
    whisper_service, model_info = model_node.load_model("small", "cuda", "float16")
    
    print("ğŸ“‹ æ¨¡å‹åŠ è½½ç»“æœ:")
    print(model_info)
    
    if whisper_service and os.path.exists("test.mp4"):
        # å…ˆæå–éŸ³é¢‘ç”¨äºæµ‹è¯•
        from audio_service import AudioService
        audio_service = AudioService()
        audio_path = "./test_whisper.wav"
        
        # ä»æµ‹è¯•è§†é¢‘æå–éŸ³é¢‘
        if audio_service.extract_audio_from_video("test.mp4", audio_path):
            print("\nğŸ™ï¸ æµ‹è¯•éŸ³é¢‘è½¬å½•...")
            
            # æµ‹è¯•è½¬å½•èŠ‚ç‚¹
            transcribe_node = WhisperTranscribeNode()
            text, lang, conf, info = transcribe_node.transcribe_audio(
                whisper_service, audio_path
            )
            
            print("ğŸ“ è½¬å½•ç»“æœ:")
            print(f"æ–‡æœ¬: {text[:100]}...")
            print(f"è¯­è¨€: {lang}")
            print(f"ç½®ä¿¡åº¦: {conf}")
            print("\n" + info)
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(audio_path)
        
        # æµ‹è¯•ç¼“å­˜ç®¡ç†
        cache_node = WhisperCacheManagerNode()
        cache_info = cache_node.manage_cache("get_info")
        print("\n" + cache_info[0])
    
    print("\nâœ… WhisperèŠ‚ç‚¹æµ‹è¯•å®Œæˆ!")